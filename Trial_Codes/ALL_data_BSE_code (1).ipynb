{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fd5a83b-21ae-4652-85cd-efd225f45ce1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:41:22.324390Z",
     "start_time": "2024-11-18T06:40:55.323524Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extract ALl Data from BSE\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "def get_equity_data(driver):\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    dict1 = dict()\n",
    "\n",
    "    div_elements = soup.find_all('div', class_='col-lg-13')\n",
    "    for element in div_elements:\n",
    "        try:\n",
    "            tr_tags = element.find('tbody').find_all('tr')\n",
    "        except:\n",
    "            continue\n",
    "        for tr_tag in tr_tags:\n",
    "            try:\n",
    "                key = tr_tag.find('td', class_='textsr').text.strip()\n",
    "                value = tr_tag.find('td', class_='textvalue ng-binding').text.strip()\n",
    "                \n",
    "                # Assign the key-value pair\n",
    "                dict1[key] = value\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # Convert the extracted dictionary to DataFrame\n",
    "    row_list = []\n",
    "    data = dict1\n",
    "    row_list.append(data.values())\n",
    "    df_equity = pd.DataFrame(row_list, columns=data.keys())\n",
    "\n",
    "    # Check if 'PE/PB' column exists in the DataFrame\n",
    "    if 'PE/PB' in df_equity.columns:\n",
    "        # Split the 'PE/PB' column into two columns 'PE' and 'PB'\n",
    "        df_equity[['PE', 'PB']] = df_equity['PE/PB'].str.split(' / ', expand=True)\n",
    "        \n",
    "        # Drop the original 'PE/PB' column\n",
    "        df_equity.drop(columns=['PE/PB'], inplace=True)\n",
    "\n",
    "    return df_equity\n",
    "\n",
    "\n",
    "def get_corpgov_data(driver):\n",
    "    try:\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.element_to_be_clickable((By.LINK_TEXT, \"Corporate Governance\"))\n",
    "        )\n",
    "\n",
    "        element = driver.find_element(By.LINK_TEXT, \"Corporate Governance\")\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "        time.sleep(1)\n",
    "        element.click()\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        element = soup.find_all('table', class_='ng-scope')[4]\n",
    "        row_list = []\n",
    "        tr_body = element.find('tbody').find_all('tr')\n",
    "\n",
    "        for tr_tag in tr_body:\n",
    "            l = []\n",
    "            try:\n",
    "                td_tags = tr_tag.find_all('td', class_='ng-binding')\n",
    "                for td_tag in td_tags:\n",
    "                    l.append(td_tag.text.strip())\n",
    "            except:\n",
    "                continue\n",
    "            row_list.append(l)\n",
    "\n",
    "        # Original column data\n",
    "        column_data = ['Sr', 'Title (Mr/Ms)', 'Name of the Director', 'DIN', 'Category', \n",
    "                       'Whether the director is disqualified?', 'Start Date of disqualification', \n",
    "                       'End Date of disqualification', 'Details of disqualification', \n",
    "                       'Current status', 'Whether special resolution passed?', \n",
    "                       'Date of passing special resolution', 'Initial Date of Appointment', \n",
    "                       'Date of Re-appointment', 'Date of cessation', 'Tenure of Director (in months)', \n",
    "                       'No of Directorship in listed entities', \n",
    "                       'No of Independent Directorship in listed entities', \n",
    "                       'Number of memberships in Audit/ Stakeholder Committee(s)', \n",
    "                       'No of post of Chairperson in Audit/ Stakeholder Committee', \n",
    "                       'Reason for Cessation', 'Notes for not providing PAN', 'Notes for not providing DIN']\n",
    "\n",
    "        dataframe = pd.DataFrame(row_list, columns=column_data)\n",
    "\n",
    "        # Combine 'Title (Mr/Ms)' and 'Name of the Director' into one column\n",
    "        dataframe['Director Name'] = dataframe['Title (Mr/Ms)'] + \" \" + dataframe['Name of the Director']\n",
    "\n",
    "        # Dropping original columns after combining\n",
    "        dataframe.drop(columns=['Title (Mr/Ms)', 'Name of the Director'], inplace=True)\n",
    "\n",
    "        # Create a filtered DataFrame with only the needed columns\n",
    "        needed_row_list = []\n",
    "        for lis in row_list:\n",
    "            combined_name = lis[1] + \" \" + lis[2]  # Combining title and name for filtered DataFrame\n",
    "            row_list1 = [lis[0], combined_name, lis[4], lis[9], lis[12], lis[13]]\n",
    "            needed_row_list.append(row_list1)\n",
    "\n",
    "        # Define the new column names for the filtered DataFrame\n",
    "        column_data1 = ['Sr', 'Director Name', 'Category', 'Current status', \n",
    "                        'Initial Date of Appointment', 'Date of Re-appointment']\n",
    "        \n",
    "        dataframe1 = pd.DataFrame(needed_row_list, columns=column_data1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_corpgov_data: {e}\")\n",
    "        dataframe = pd.DataFrame()\n",
    "        dataframe1 = pd.DataFrame()\n",
    "\n",
    "    return dataframe, dataframe1\n",
    "\n",
    "    \n",
    "def get_quarterly_data(driver):\n",
    "    try:\n",
    "        table_html = driver.find_element(By.CSS_SELECTOR, 'div.tab-pane.active.largetable').get_attribute('outerHTML')\n",
    "        soup = BeautifulSoup(table_html, 'html.parser')\n",
    "        headers = [header.text.strip() for header in soup.find_all('td', class_='tableheading')]\n",
    "        \n",
    "        rows = []\n",
    "        for row in soup.find_all('tr'):\n",
    "            cells = [cell.text.strip() for cell in row.find_all('td', class_='tdcolumn')]\n",
    "            if cells:\n",
    "                rows.append(cells)\n",
    "        \n",
    "        consistent_rows = [row for row in rows if len(row) == len(headers)]\n",
    "        df_quarterly = pd.DataFrame(consistent_rows, columns=headers)\n",
    "        \n",
    "        # Transpose the dataframe to make it more manageable\n",
    "        df_quarterly_transposed = df_quarterly.T\n",
    "        new_header = df_quarterly_transposed.iloc[0]\n",
    "        df_quarterly_transposed = df_quarterly_transposed[1:]\n",
    "        df_quarterly_transposed.columns = new_header\n",
    "        \n",
    "        # Rename the 'Results (in Cr.)' column\n",
    "        df_quarterly_transposed.rename(columns={'Results (in Cr.)  View in (Million)': 'Year'}, inplace=True)\n",
    "        \n",
    "        # Split the '52 W H/L' column into '52 W H' and '52 W L'\n",
    "        if '52 W H/L' in df_quarterly_transposed.columns:\n",
    "            df_quarterly_transposed[['52 W H', '52 W L']] = df_quarterly_transposed['52 W H/L'].str.split('/', expand=True)\n",
    "            df_quarterly_transposed.drop(columns=['52 W H/L'], inplace=True)\n",
    "        \n",
    "        df_quarterly_transposed.reset_index(inplace=True)\n",
    "        df_quarterly_transposed.rename(columns={'index': 'Peer Company'}, inplace=True)\n",
    "\n",
    "        # Filter the dataframe to include only the required columns\n",
    "        required_columns = [\n",
    "            'Peer Company', 'LTP', 'Change %', 'Year', 'Sales', 'PAT', 'Equity', \n",
    "            'Face Value', 'OPM %', 'NPM %', 'EPS', 'CEPS', 'PE', \n",
    "            '52 W H', '52 W L'\n",
    "        ]\n",
    "        \n",
    "        df_quarterly_filtered = df_quarterly_transposed[required_columns]\n",
    "\n",
    "        return df_quarterly_filtered\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_quarterly_data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def get_annual_data(driver):\n",
    "    try:\n",
    "        annual_trends_button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.ID, 'aanualtrd')))\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", annual_trends_button)\n",
    "        time.sleep(1)\n",
    "        annual_trends_button.click()\n",
    "        time.sleep(5)\n",
    "        \n",
    "        table_html = driver.find_element(By.CSS_SELECTOR, 'div.tab-pane.active.largetable').get_attribute('outerHTML')\n",
    "        soup = BeautifulSoup(table_html, 'html.parser')\n",
    "        headers = [header.text.strip() for header in soup.find_all('td', class_='tableheading')]\n",
    "        \n",
    "        rows = []\n",
    "        for row in soup.find_all('tr'):\n",
    "            cells = [cell.text.strip() for cell in row.find_all('td', class_='tdcolumn')]\n",
    "            if cells:\n",
    "                rows.append(cells)\n",
    "        \n",
    "        consistent_rows = [row for row in rows if len(row) == len(headers)]\n",
    "        df_annual = pd.DataFrame(consistent_rows, columns=headers)\n",
    "        \n",
    "        # Transpose the dataframe to make it more manageable\n",
    "        df_annual_transposed = df_annual.T\n",
    "        new_header = df_annual_transposed.iloc[0]\n",
    "        df_annual_transposed = df_annual_transposed[1:]\n",
    "        df_annual_transposed.columns = new_header\n",
    "        \n",
    "        # Rename the 'Results (in Cr.)' column\n",
    "        df_annual_transposed.rename(columns={'Results (in Cr.)  View in (Million)': 'Year'}, inplace=True)\n",
    "        \n",
    "        # Split the '52 W H/L' column into '52 W H' and '52 W L'\n",
    "        if '52 W H/L' in df_annual_transposed.columns:\n",
    "            df_annual_transposed[['52 W H', '52 W L']] = df_annual_transposed['52 W H/L'].str.split('/', expand=True)\n",
    "            df_annual_transposed.drop(columns=['52 W H/L'], inplace=True)\n",
    "        \n",
    "        df_annual_transposed.reset_index(inplace=True)\n",
    "        df_annual_transposed.rename(columns={'index': 'Peer Company'}, inplace=True)\n",
    "\n",
    "        # Filter the dataframe to include only the required columns\n",
    "        required_columns = [\n",
    "            'Peer Company', 'LTP', 'Change %', 'Year', 'Sales', 'PAT', 'Equity', \n",
    "            'Face Value', 'OPM %', 'NPM %', 'EPS', 'CEPS', 'PE', \n",
    "            '52 W H', '52 W L'\n",
    "        ]\n",
    "        \n",
    "        df_annual_filtered = df_annual_transposed[required_columns]\n",
    "\n",
    "        return df_annual_filtered\n",
    "    except Exception as e:\n",
    "        print(f\"Error in get_annual_data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def main(url):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    service = Service()  # Add chromedriver path if needed\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        \n",
    "        def click_element(driver, xpath):\n",
    "            try:\n",
    "                element = WebDriverWait(driver, 10).until(\n",
    "                    EC.element_to_be_clickable((By.XPATH, xpath))\n",
    "                )\n",
    "                driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "                ActionChains(driver).move_to_element(element).click().perform()\n",
    "            except Exception as e:\n",
    "                print(f\"Click failed: {e}\")\n",
    "\n",
    "        click_element(driver, \"//div[@id='l14']\")\n",
    "        click_element(driver, \"//a[contains(text(), 'Peer Group')]\")\n",
    "        \n",
    "        time.sleep(5)\n",
    "        \n",
    "        df_quarterly = get_quarterly_data(driver)\n",
    "        df_annual = get_annual_data(driver)\n",
    "        # Extract company name from URL\n",
    "        l = url.split('/')\n",
    "        company_name = l[4].strip().capitalize() #Pass the company name here\n",
    "        df_corpgov, df_corpgov_filtered = get_corpgov_data(driver)\n",
    "        df_equity = get_equity_data(driver)\n",
    "\n",
    "        # Add company name and industry to the DataFrames\n",
    "        for df in [df_equity, df_corpgov, df_corpgov_filtered, df_quarterly, df_annual]:\n",
    "            df['Company Name'] = company_name\n",
    "            df['Industry Name'] = \"Auto Components & Equipments\"\n",
    "\n",
    "        # Define the Excel file path\n",
    "        excel_path = \"Combined_data_BSE.xlsx\"\n",
    "\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(excel_path):\n",
    "            # Load existing data\n",
    "            with pd.ExcelFile(excel_path) as xls:\n",
    "                df_quarterly_existing = pd.read_excel(xls, sheet_name='Peer_Quarterly Data')\n",
    "                df_annual_existing = pd.read_excel(xls, sheet_name='Peer_Annual Data')\n",
    "                df_corpgov_existing = pd.read_excel(xls, sheet_name='Corporate Governance')\n",
    "                df_corpgov_filtered_existing = pd.read_excel(xls, sheet_name='Board_Members')\n",
    "                df_equity_existing = pd.read_excel(xls, sheet_name='Equity Data')\n",
    "\n",
    "            # Append new data to existing data\n",
    "            df_quarterly = pd.concat([df_quarterly_existing, df_quarterly], ignore_index=True)\n",
    "            df_annual = pd.concat([df_annual_existing, df_annual], ignore_index=True)\n",
    "            df_corpgov = pd.concat([df_corpgov_existing, df_corpgov], ignore_index=True)\n",
    "            df_corpgov_filtered = pd.concat([df_corpgov_filtered_existing, df_corpgov_filtered], ignore_index=True)\n",
    "            df_equity = pd.concat([df_equity_existing, df_equity], ignore_index=True)\n",
    "\n",
    "        # Save all DataFrames to different sheets in an Excel file\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl', mode='w') as writer:\n",
    "            df_quarterly.to_excel(writer, sheet_name='Peer_Quarterly Data', index=False)\n",
    "            df_annual.to_excel(writer, sheet_name='Peer_Annual Data', index=False)\n",
    "            df_corpgov.to_excel(writer, sheet_name='Corporate Governance', index=False)\n",
    "            df_corpgov_filtered.to_excel(writer, sheet_name='Board_Members', index=False)\n",
    "            df_equity.to_excel(writer, sheet_name='Equity Data', index=False)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Update this URL with the one you want to scrape\n",
    "    urls = [\n",
    "        #'https://www.bseindia.com/stock-share-price/shivam-autotech-ltd/shivamauto/532776/',\n",
    "       #'https://www.bseindia.com/stock-share-price/fiem-industries-ltd/fiemind/532768/',\n",
    "          #'https://www.bseindia.com/stock-share-price/india-nippon-electricals-ltd/indnippon/532240/',\n",
    "        #'https://www.bseindia.com/stock-share-price/lumax-auto-technologies-ltd/lumaxtech/532796/',\n",
    "           #'https://www.bseindia.com/stock-share-price/sandhar-technologies-ltd/sandhar/541163/',\n",
    "         #'https://www.bseindia.com/stock-share-price/nrb-bearings-ltd/nrbbearing/530367/',\n",
    "        #'https://www.bseindia.com/stock-share-price/mmforgings-ltd/mmfl/522241/',\n",
    "        #'https://www.bseindia.com/stock-share-price/steel-strips-wheels-ltd/sswl/513262/',\n",
    "        #'https://www.bseindia.com/stock-share-price/igarashi-motors-india-ltd/igarashi/517380/',\n",
    "        #'https://www.bseindia.com/stock-share-price/jay-bharat-maruti-ltd/jaybarmaru/520066/',\n",
    "        #'https://www.bseindia.com/stock-share-price/g-n-a-axles-ltd/gna/540124/',\n",
    "       'https://www.bseindia.com/stock-share-price/talbros-automotive-components-ltd/talbroauto/505160/',\n",
    "       # 'https://www.bseindia.com/stock-share-price/lumax-industries-ltd/lumaxind/517206/',\n",
    "       # 'https://www.bseindia.com/stock-share-price/wheels-india-ltd/wheels/590073/',\n",
    "       # 'https://www.bseindia.com/stock-share-price/bharat-seats-ltd/bharatse/523229/',\n",
    "        #'https://www.bseindia.com/stock-share-price/alicon-castalloy-limited/alicon/531147/',\n",
    "        \n",
    "        \n",
    "    ]\n",
    "\n",
    "    for url in urls:\n",
    "        main(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36713b79-ed09-46bb-b49e-3e5d9f17e968",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:42:13.110884Z",
     "start_time": "2024-11-18T06:41:22.326726Z"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "\n",
    "def get_financial_results(driver, company, industry, excel_path):\n",
    "    try:\n",
    "        # Expand the financial results section\n",
    "        element_to_click = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a#afi[data-toggle='collapse'][data-parent='#accordion']\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element_to_click)\n",
    "        driver.execute_script(\"arguments[0].click();\", element_to_click)\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Click on Quarterly Trends\n",
    "        quarterly_element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CSS_SELECTOR, \"a#l61\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", quarterly_element)\n",
    "\n",
    "        time.sleep(6)\n",
    "\n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        tables = soup.find_all('table', class_=\"ng-binding\")\n",
    "\n",
    "        if not tables:\n",
    "            print(\"No tables found with class 'ng-binding'\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        # Process Quarterly Trends\n",
    "        financial_quarterly_df = process_table(tables[0], 'Quarterly')\n",
    "        financial_quarterly_df['Company Name'] = company\n",
    "        financial_quarterly_df['Industry Name'] = industry\n",
    "\n",
    "        # Click on Annual Trends\n",
    "        annual_element = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.LINK_TEXT, \"Annual Trends\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].click();\", annual_element)\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "        # Parse Annual Trends data\n",
    "        soup1 = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        tables1 = soup1.find_all('table', class_=\"ng-binding\")\n",
    "\n",
    "        if len(tables1) < 4:\n",
    "            print(f\"Expected at least 4 tables, but found {len(tables1)}\")\n",
    "            return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        # Process Annual Trends\n",
    "        financial_annual_df = process_table(tables1[3], 'Annual')\n",
    "        financial_annual_df['Company Name'] = company\n",
    "        financial_annual_df['Industry Name'] = industry\n",
    "\n",
    "        return financial_quarterly_df, financial_annual_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {company}: {str(e)}\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "def process_table(table, trend_type):\n",
    "    row_list = []\n",
    "    columns = []\n",
    "    \n",
    "    try:\n",
    "        tr_heads = table.find('thead').find_all('tr')\n",
    "        tr_body = table.find('tbody').find_all('tr')\n",
    "    except AttributeError:\n",
    "        print(f\"Table structure for {trend_type} Trends is not as expected\")\n",
    "        return pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    # Extract data from body rows\n",
    "    for tr_tag in tr_body:\n",
    "        l = []\n",
    "        try:\n",
    "            td_tags = tr_tag.find_all('td', class_='tdcolumn')\n",
    "            for td_tag in td_tags:\n",
    "                l.append(td_tag.text.strip())\n",
    "        except:\n",
    "            continue\n",
    "        row_list.append(l)\n",
    "    \n",
    "    # Extract column headers\n",
    "    for th_tag in tr_heads:\n",
    "        td_tags = th_tag.find_all('td', class_='tableheading')\n",
    "        for td_tag in td_tags:\n",
    "            columns.append(td_tag.text.strip())\n",
    "    \n",
    "    dataframe = pd.DataFrame(row_list, columns=columns)\n",
    "    dataframe = dataframe[:-3]  # Remove unwanted rows\n",
    "    dataframe = dataframe.drop(dataframe.index[0])  # Remove header row\n",
    "    if trend_type == 'Quarterly':\n",
    "        if dataframe.columns[-1].startswith('FY'):\n",
    "            dataframe = dataframe.iloc[:, :-1]  # Drop the last column\n",
    "    \n",
    "    # Transpose the data\n",
    "    dataframe = dataframe.set_index(dataframe.columns[0]).transpose()\n",
    "    dataframe.reset_index(inplace=True)\n",
    "    dataframe.rename(columns={'index': 'Date' if trend_type == 'Quarterly' else 'Year'}, inplace=True)\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def scrape_meetings(driver, url, industry_name):\n",
    "    try:\n",
    "        # Extract company name from URL\n",
    "        l = url.split('/')\n",
    "        company_name = l[4].strip().capitalize()\n",
    "\n",
    "        # Open the main page\n",
    "        driver.get(url)\n",
    "\n",
    "        # Expand the meetings section\n",
    "        element_to_click = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a#ame[data-toggle='collapse'][data-parent='#accordion']\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element_to_click)\n",
    "        driver.execute_script(\"arguments[0].click();\", element_to_click)\n",
    "\n",
    "        time.sleep(3)  # Wait for the section to expand\n",
    "\n",
    "        # Function to extract table data for meetings\n",
    "        def extract_meeting_data(link_id, state_attr, meeting_type):\n",
    "            meetings_link = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.ID, link_id))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", meetings_link)\n",
    "\n",
    "            time.sleep(6)  # Wait for the page to load\n",
    "\n",
    "            # Parse the page content with BeautifulSoup\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # Locate the table for the meeting data\n",
    "            table = soup.find('table', {'ng-if': f\"loader.{state_attr}=='loaded'\"})\n",
    "            data = []\n",
    "\n",
    "            if table:\n",
    "                rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "                for row in rows:\n",
    "                    cols = row.find_all('td')\n",
    "                    if len(cols) >= 2:  # Ensure there are at least 2 columns\n",
    "                        meeting_date = cols[0].text.strip()\n",
    "                        purpose = cols[1].text.strip()\n",
    "                        data.append([meeting_date, purpose, meeting_type, company_name, industry_name])\n",
    "            else:\n",
    "                print(f\"Table for {state_attr} not found.\")\n",
    "\n",
    "            # Navigate back to the main page\n",
    "            driver.back()\n",
    "            time.sleep(3)  # Wait for the page to load again\n",
    "\n",
    "            return data\n",
    "\n",
    "        # Step 1: Scrape Board Meetings Data\n",
    "        board_meetings_data = extract_meeting_data(\"l71\", \"BMState\", \"Board Meeting\")\n",
    "        board_meetings_df = pd.DataFrame(board_meetings_data, columns=['Meeting Date', 'Purpose', 'Meeting Type', 'Company Name', 'Industry'])\n",
    "\n",
    "        # Step 2: Scroll again to find and click the Shareholder Meetings link\n",
    "        element_to_click = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"a#ame[data-toggle='collapse'][data-parent='#accordion']\"))\n",
    "        )\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element_to_click)\n",
    "        driver.execute_script(\"arguments[0].click();\", element_to_click)\n",
    "\n",
    "        time.sleep(3)  # Wait for the section to expand again\n",
    "\n",
    "        # Step 3: Scrape Shareholder Meetings Data\n",
    "        shareholder_meetings_data = extract_meeting_data(\"l72\", \"SHState\", \"Shareholder Meeting\")\n",
    "        shareholder_meetings_df = pd.DataFrame(shareholder_meetings_data, columns=['Meeting Date', 'Purpose', 'Meeting Type', 'Company Name', 'Industry'])\n",
    "\n",
    "        # Combine both dataframes into one\n",
    "        combined_meetings_df = pd.concat([board_meetings_df, shareholder_meetings_df], ignore_index=True)\n",
    "\n",
    "        return combined_meetings_df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n",
    "def main():\n",
    "    # Configure Selenium WebDriver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run browser in headless mode\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "    service = Service()  # Add chromedriver path if needed\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    # List of URLs to scrape\n",
    "    urls = [\n",
    "        #'https://www.bseindia.com/stock-share-price/shivam-autotech-ltd/shivamauto/532776/',\n",
    "        #'https://www.bseindia.com/stock-share-price/fiem-industries-ltd/fiemind/532768/',\n",
    "        #'https://www.bseindia.com/stock-share-price/india-nippon-electricals-ltd/indnippon/532240/',\n",
    "        #'https://www.bseindia.com/stock-share-price/lumax-auto-technologies-ltd/lumaxtech/532796/',\n",
    "        #'https://www.bseindia.com/stock-share-price/sandhar-technologies-ltd/sandhar/541163/',\n",
    "         #'https://www.bseindia.com/stock-share-price/nrb-bearings-ltd/nrbbearing/530367/',\n",
    "        #'https://www.bseindia.com/stock-share-price/mmforgings-ltd/mmfl/522241/',\n",
    "        #'https://www.bseindia.com/stock-share-price/steel-strips-wheels-ltd/sswl/513262/',\n",
    "        #'https://www.bseindia.com/stock-share-price/igarashi-motors-india-ltd/igarashi/517380/',\n",
    "        #'https://www.bseindia.com/stock-share-price/jay-bharat-maruti-ltd/jaybarmaru/520066/',\n",
    "        #'https://www.bseindia.com/stock-share-price/g-n-a-axles-ltd/gna/540124/',\n",
    "        'https://www.bseindia.com/stock-share-price/talbros-automotive-components-ltd/talbroauto/505160/',\n",
    "        #'https://www.bseindia.com/stock-share-price/lumax-industries-ltd/lumaxind/517206/',\n",
    "        #'https://www.bseindia.com/stock-share-price/wheels-india-ltd/wheels/590073/',\n",
    "        #'https://www.bseindia.com/stock-share-price/bharat-seats-ltd/bharatse/523229/',\n",
    "        #'https://www.bseindia.com/stock-share-price/alicon-castalloy-limited/alicon/531147/',\n",
    "    ]\n",
    "\n",
    "    # Specify the path where you want to save the Excel file\n",
    "    excel_path = \"Financial_Bse_data.xlsx\"\n",
    "\n",
    "    all_financial_quarterly_data = pd.DataFrame()  # Changed name\n",
    "    all_financial_annual_data = pd.DataFrame()      # Changed name\n",
    "    all_meetings_data = pd.DataFrame()              # Unchanged\n",
    "\n",
    "    for link in urls:\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Extract company name from URL\n",
    "        l = link.split('/')\n",
    "        company = l[4].strip().capitalize()\n",
    "\n",
    "        # Get financial results with industry name\n",
    "        financial_quarterly_data, financial_annual_data = get_financial_results(driver, company, \"Auto Components & Equipments\", excel_path)\n",
    "\n",
    "        all_financial_quarterly_data = pd.concat([all_financial_quarterly_data, financial_quarterly_data], ignore_index=True)\n",
    "        all_financial_annual_data = pd.concat([all_financial_annual_data, financial_annual_data], ignore_index=True)\n",
    "\n",
    "        # Get meeting data\n",
    "        meetings_data = scrape_meetings(driver, link, \"Auto Components & Equipments\")\n",
    "        all_meetings_data = pd.concat([all_meetings_data, meetings_data], ignore_index=True)\n",
    "\n",
    "    # Check if the Excel file exists\n",
    "    if os.path.exists(excel_path):\n",
    "        # Load existing data from Excel\n",
    "        with pd.ExcelFile(excel_path) as xls:\n",
    "            # Read existing data from sheets\n",
    "            existing_quarterly_df = pd.read_excel(xls, sheet_name='Financial_Quarterly Trend')\n",
    "            existing_annual_df = pd.read_excel(xls, sheet_name='Financial_Annual Trend')\n",
    "            existing_meetings_df = pd.read_excel(xls, sheet_name='Meeting_Data')\n",
    "\n",
    "            # Append new data to existing data\n",
    "            all_financial_quarterly_data = pd.concat([existing_quarterly_df, all_financial_quarterly_data], ignore_index=True)\n",
    "            all_financial_annual_data = pd.concat([existing_annual_df, all_financial_annual_data], ignore_index=True)\n",
    "            all_meetings_data = pd.concat([existing_meetings_df, all_meetings_data], ignore_index=True)\n",
    "\n",
    "    # Save the collected data to Excel\n",
    "    with pd.ExcelWriter(excel_path) as writer:\n",
    "        all_financial_quarterly_data.to_excel(writer, sheet_name='Financial_Quarterly Trend', index=False)  # Changed sheet name\n",
    "        all_financial_annual_data.to_excel(writer, sheet_name='Financial_Annual Trend', index=False)      # Changed sheet name\n",
    "        all_meetings_data.to_excel(writer, sheet_name='Meeting_Data', index=False)\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8654d61-f710-41e1-9ab2-1eedae7ad557",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:42:13.117450Z",
     "start_time": "2024-11-18T06:42:13.113149Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
